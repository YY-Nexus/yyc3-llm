import { useState, useEffect } from "react"

// ç±»å‹å®šä¹‰
export type ModelType = 'chat' | 'code' | 'multimodal'
export type ModelProvider = "ollama" | "openai" | "anthropic" | "google"




export interface ModelManagementConfig {
  ollamaUrl: string
  modelCacheDir: string
  defaultModels: Array<{
    id: string
    name: string
    type: ModelType
    provider: string
  }>
  autoDownload: boolean
  maxConcurrentDownloads: number
  connectionTimeout: number
  retryAttempts: number
}

export interface AIModel {
  id: string
  name: string
  type: ModelType
  provider: ModelProvider
  status: 'ready' | 'downloading' | 'not_downloaded' | 'download_failed' | 'unavailable' | 'unknown'
  size: number
  lastUsed: string | Date | null
  parameters: string
  quantization: string
  createdAt: Date
  updatedAt?: Date
  error?: string
  usageCount?: number
  downloadProgress?: number
}

export interface ModelTask {
  id: string;
  modelId: string;
  type: 'download' | 'delete' | 'update';
  status: 'pending' | 'downloading' | 'completed' | 'failed';
  progress: number;
  createdAt: Date;
  updatedAt: Date;
  startedAt?: Date;
  completedAt?: Date;
  error?: string;
}

// AIæ¨¡å‹ç®¡ç†ä¸­å¿ƒ - ç»Ÿä¸€ç®¡ç†æœ¬åœ°å’Œäº‘ç«¯AIæ¨¡å‹
export class ModelManagementCenter {
  private static instance: ModelManagementCenter
  private models = new Map<string, AIModel>()
  private modelTasks = new Map<string, ModelTask>()
  private config: ModelManagementConfig
  private _connectionStatus: 'connected' | 'error' | 'unknown' = 'unknown';
  private _errorMessage: string | null = null;
  
  // Getters for connection status
  public get connectionStatus(): 'connected' | 'error' | 'unknown' {
    return this._connectionStatus;
  }
  
  public get errorMessage(): string | null {
    return this._errorMessage;
  }
  
  // åˆå§‹åŒ–æ¨¡å‹åˆ—è¡¨
  private async initializeModels(): Promise<void> {
    try {
      // è·å–æœ¬åœ°å·²å®‰è£…çš„Ollamaæ¨¡å‹
      await this.fetchOllamaModels();

      // æ·»åŠ é»˜è®¤æ¨¡å‹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
      for (const defaultModel of this.config.defaultModels) {
        if (!this.models.has(defaultModel.id)) {
          // ç¡®ä¿provideræ˜¯æœ‰æ•ˆçš„ModelProviderç±»å‹
          const provider = defaultModel.provider as ModelProvider;
          this.models.set(defaultModel.id, {
            ...defaultModel,
            status: "not_downloaded",
            size: 0,
            lastUsed: null,
            parameters: defaultModel.id.includes("7b") ? "7B" : defaultModel.id.includes("8b") ? "8B" : "Unknown",
            quantization: defaultModel.id.includes("q4_0") ? "Q4_0" : "None",
            createdAt: new Date(),
            provider // ä½¿ç”¨ç±»å‹æ–­è¨€åçš„provider
          })
        }
      }

      console.log(`âœ… å·²åˆå§‹åŒ–${this.models.size}ä¸ªAIæ¨¡å‹`)
    } catch (error) {
      console.error("åˆå§‹åŒ–æ¨¡å‹å¤±è´¥:", error)
    }
  }
  
  private constructor() {
    // å®‰å…¨åœ°è·å–ç¯å¢ƒå˜é‡å¹¶æ·»åŠ é»˜è®¤å€¼
    const ollamaUrl = process.env.NEXT_PUBLIC_OLLAMA_URL 
      ? process.env.NEXT_PUBLIC_OLLAMA_URL.trim() 
      : "http://localhost:11434";
      
    this.config = {
      ollamaUrl,
      modelCacheDir: "/tmp/yanyu-models",
      defaultModels: [
        { id: "codellama:7b", name: "CodeLlama 7B", type: "code", provider: "ollama" },
        { id: "llama3:8b", name: "Llama 3 8B", type: "chat", provider: "ollama" },
        { id: "phi3:mini", name: "Phi-3 Mini", type: "chat", provider: "ollama" },
      ],
      autoDownload: false,
      maxConcurrentDownloads: 1,
      // æ·»åŠ è¿æ¥é…ç½®
      connectionTimeout: 5000, // 5ç§’è¶…æ—¶
      retryAttempts: 2,
    }
    
    // å¼‚æ­¥åˆå§‹åŒ–ï¼Œä¸é˜»å¡æ„é€ å‡½æ•°
    this.initializeModels().catch(err => {
      console.error("æ¨¡å‹åˆå§‹åŒ–å¤±è´¥:", err);
    })
  }

  // åˆ·æ–°æ¨¡å‹åˆ—è¡¨
  public async refreshModels(): Promise<void> {
    this._connectionStatus = 'unknown';
    this._errorMessage = null;
    await this.fetchOllamaModels();
  }

  // è·å–æ¨èæ¨¡å‹
  public getRecommendedModels(type: ModelType, limit: number = 3): AIModel[] {
    return this.getModelsByType(type)
      .filter(model => model.status === 'ready')
      .sort((a, b) => {
        // ä¼˜å…ˆæ’åºæœ‰lastUsedçš„æ¨¡å‹
        if (a.lastUsed && b.lastUsed) {
          return new Date(b.lastUsed).getTime() - new Date(a.lastUsed).getTime();
        }
        return a.lastUsed ? -1 : 1;
      })
      .slice(0, limit);
  }

  // åˆ é™¤æ¨¡å‹


  // è·å–æ‰€æœ‰ä»»åŠ¡
  public getAllTasks(): ModelTask[] {
    return Array.from(this.modelTasks.values());
  }

  


  // æ¨¡æ‹ŸåŠ è½½æ¨¡å‹ï¼ˆç”¨äºæµ‹è¯•ï¼‰
  private simulateModelDownload(modelId: string, task: ModelTask): void {
    // ä»…ç”¨äºå¼€å‘æµ‹è¯•
    let progress = 0;
    const interval = setInterval(() => {
      progress += Math.random() * 10;
      if (progress >= 100) {
        progress = 100;
        clearInterval(interval);
        
        task.status = 'completed';
        task.progress = 100;
        task.completedAt = new Date();
        
        // æ›´æ–°æ¨¡å‹çŠ¶æ€
        const model = this.models.get(modelId);
        if (model) {
          this.models.set(modelId, {
            ...model,
            status: 'ready',
            size: Math.floor(Math.random() * 1000000000), // éšæœºå¤§å°
            updatedAt: new Date()
          });
        }
      }
      
      task.progress = Math.min(progress, 100);
      task.updatedAt = new Date();
      this.modelTasks.set(modelId, { ...task });
    }, 500);
  }
  
  public static getInstance(): ModelManagementCenter {
    if (!ModelManagementCenter.instance) {
      ModelManagementCenter.instance = new ModelManagementCenter();
    }
    return ModelManagementCenter.instance;
  }

  



  // è·å–Ollamaæ¨¡å‹åˆ—è¡¨
  private async fetchOllamaModels(): Promise<void> {
    const maxRetries = 3; // æœ€å¤§é‡è¯•æ¬¡æ•°
    const retryDelay = 1000; // é‡è¯•é—´éš”ï¼ˆæ¯«ç§’ï¼‰
    let lastError: Error | null = null;

    for (let attempt = 0; attempt < maxRetries; attempt++) {
      try {
        // æ·»åŠ è¿æ¥è¶…æ—¶å¤„ç†
        const controller = new AbortController();
        const timeoutId = setTimeout(() => controller.abort(), 5000); // 5ç§’è¶…æ—¶
        
        console.log(`[å°è¯• ${attempt + 1}/${maxRetries}] è¿æ¥åˆ°OllamaæœåŠ¡: ${this.config.ollamaUrl}`);
        
        const response = await fetch(`${this.config.ollamaUrl}/api/tags`, {
          signal: controller.signal,
          headers: {
            'Content-Type': 'application/json',
            'Accept': 'application/json',
          },
          method: 'GET',
        })
        
        clearTimeout(timeoutId); // æ¸…é™¤è¶…æ—¶å®šæ—¶å™¨

        if (!response.ok) {
          const errorMessage = `è·å–Ollamaæ¨¡å‹å¤±è´¥: ${response.status} ${response.statusText}`;
          console.error(`[å°è¯• ${attempt + 1}] ${errorMessage}`);
          
          // å¦‚æœæ˜¯æœåŠ¡ç«¯é”™è¯¯ä¸”ä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œè¿›è¡Œé‡è¯•
          if (response.status >= 500 && attempt < maxRetries - 1) {
            console.log(`[å°è¯• ${attempt + 1}] æœåŠ¡ç«¯é”™è¯¯ï¼Œ${retryDelay}msåé‡è¯•...`);
            await new Promise(resolve => setTimeout(resolve, retryDelay));
            continue;
          }
          
          throw new Error(errorMessage);
        }

        try {
          const data = await response.json();
          
          if (data.models && Array.isArray(data.models)) {
            console.log(`[æˆåŠŸ] è·å–åˆ° ${data.models.length} ä¸ªOllamaæ¨¡å‹`);
            
            for (const model of data.models) {
              const modelId = model.name;
              const existingModel = this.models.get(modelId);

              this.models.set(modelId, {
                id: modelId,
                name: this.formatModelName(modelId),
                type: this.inferModelType(modelId),
                provider: "ollama",
                status: "ready",
                size: model.size || 0,
                lastUsed: existingModel?.lastUsed || null,
                parameters: this.inferModelParameters(modelId),
                quantization: this.inferModelQuantization(modelId),
                createdAt: existingModel?.createdAt || new Date(),
                updatedAt: new Date(),
                error: undefined, // æ¸…é™¤ä¹‹å‰çš„é”™è¯¯ä¿¡æ¯
              });
            }
          } else {
            console.warn("Ollama API è¿”å›äº†æ— æ•ˆçš„æ¨¡å‹æ•°æ®ç»“æ„");
          }
          
          // è®¾ç½®è¿æ¥çŠ¶æ€ä¸ºå·²è¿æ¥
          this.setConnectionStatus('connected');
          return; // æˆåŠŸè·å–ï¼Œç›´æ¥è¿”å›
        } catch (jsonError) {
          console.error(`[å°è¯• ${attempt + 1}] è§£æOllama APIå“åº”å¤±è´¥:`, jsonError);
          throw new Error('è§£æOllamaæ¨¡å‹æ•°æ®å¤±è´¥');
        }
      } catch (error) {
        lastError = error as Error;
        console.error(`[å°è¯• ${attempt + 1}] è·å–Ollamaæ¨¡å‹å¤±è´¥:`, error);
        
        // å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•æˆ–ä¸æ˜¯ç½‘ç»œç›¸å…³é”™è¯¯ï¼Œä¸å†é‡è¯•
        if (attempt >= maxRetries - 1 || 
            (lastError && !(lastError.name === 'AbortError' || 
                           lastError.message.includes('Network') ||
                           lastError.message.includes('fetch')))) {
          break;
        }
        
        console.log(`[å°è¯• ${attempt + 1}] è¿æ¥å¤±è´¥ï¼Œ${retryDelay}msåé‡è¯•...`);
        await new Promise(resolve => setTimeout(resolve, retryDelay));
      }
    }
    
    // æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè®¾ç½®é”™è¯¯çŠ¶æ€
      if (lastError) {
        // è®¾ç½®è¿æ¥çŠ¶æ€ä¸ºé”™è¯¯
        this.setConnectionStatus('error', lastError.name === 'AbortError' ? 
          'è¿æ¥OllamaæœåŠ¡è¶…æ—¶ï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ' : 
          'æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡ï¼Œè¯·ç¡®ä¿Ollamaå·²å®‰è£…å¹¶æ­£åœ¨è¿è¡Œ');
        
        // æ ¹æ®é”™è¯¯ç±»å‹æä¾›æ›´å…·ä½“çš„é”™è¯¯ä¿¡æ¯
        let errorMessage: string;
      
      if (lastError.name === 'AbortError') {
        errorMessage = 'è¿æ¥OllamaæœåŠ¡è¶…æ—¶ï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ';
      } else if (lastError.message.includes('Network') || lastError.message.includes('fetch')) {
        errorMessage = 'ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦å¯ç”¨';
      } else {
        errorMessage = `OllamaæœåŠ¡é”™è¯¯: ${lastError.message}`;
      }
      
      console.error(`[æœ€ç»ˆ] ${errorMessage}`);
      
      // æ›´æ–°æ‰€æœ‰Ollamaæ¨¡å‹çŠ¶æ€
      for (const [id, model] of this.models.entries()) {
        if (model.provider === "ollama") {
          this.models.set(id, { 
            ...model, 
            status: "unavailable",
            error: errorMessage,
            updatedAt: new Date()
          });
        }
      }
      
      // åŒæ—¶æ£€æŸ¥æ˜¯å¦æœ‰å·²å®‰è£…ä½†æœªåœ¨æ¨¡å‹åˆ—è¡¨ä¸­çš„æ¨¡å‹
      this.scanForOllamaModels();
    }
  }
  
  // æ‰«æOllamaå·²å®‰è£…ä½†æœªåœ¨æ¨¡å‹åˆ—è¡¨ä¸­çš„æ¨¡å‹
  private scanForOllamaModels(): void {
    // è¿™é‡Œå¯ä»¥æ·»åŠ é€»è¾‘ï¼Œä»å…¶ä»–æ¥æºï¼ˆå¦‚æœ¬åœ°é…ç½®ï¼‰è·å–å·²çŸ¥çš„Ollamaæ¨¡å‹
    // ç›®å‰ä»…ä½œä¸ºå ä½æ–¹æ³•
    console.log('æ‰«æå¯èƒ½çš„Ollamaæ¨¡å‹...');
  }

  // æ ¼å¼åŒ–æ¨¡å‹åç§°
  private formatModelName(modelId: string): string {
    // å°†æ¨¡å‹IDè½¬æ¢ä¸ºæ›´å‹å¥½çš„æ˜¾ç¤ºåç§°
    const parts = modelId.split(':');
    const baseName = parts[0];
    const version = parts[1] || '';

    const nameMap: Record<string, string> = {
      codellama: 'CodeLlama',
      llama3: 'Llama 3',
      llama2: 'Llama 2',
      phi3: 'Phi-3',
      mistral: 'Mistral',
      qwen: 'Qwen',
      gemma: 'Gemma'
    };

    const formattedName = nameMap[baseName] || baseName.charAt(0).toUpperCase() + baseName.slice(1);

    if (version) {
      return `${formattedName} ${version}`;
    }

    return formattedName;
  }

  // æ¨æ–­æ¨¡å‹ç±»å‹
  private inferModelType(modelId: string): ModelType {
    const id = modelId.toLowerCase()

    if (id.includes("code") || id.includes("starcoder") || id.includes("deepseek-coder")) {
      return "code"
    } else if (id.includes("vision") || id.includes("clip") || id.includes("image")) {
      return "multimodal"
    } else {
      return "chat"
    }
  }

  // æ¨æ–­æ¨¡å‹å‚æ•°é‡
  private inferModelParameters(modelId: string): string {
    const id = modelId.toLowerCase()

    if (id.includes("70b")) return "70B"
    if (id.includes("34b")) return "34B"
    if (id.includes("13b")) return "13B"
    if (id.includes("8b")) return "8B"
    if (id.includes("7b")) return "7B"
    if (id.includes("3b")) return "3B"
    if (id.includes("1b")) return "1B"

    return "æœªçŸ¥"
  }

  // æ¨æ–­æ¨¡å‹é‡åŒ–æ–¹å¼
  private inferModelQuantization(modelId: string): string {
    const id = modelId.toLowerCase()

    if (id.includes("q4_0")) return "Q4_0"
    if (id.includes("q4_1")) return "Q4_1"
    if (id.includes("q5_0")) return "Q5_0"
    if (id.includes("q5_1")) return "Q5_1"
    if (id.includes("q8_0")) return "Q8_0"

    return "æ— é‡åŒ–"
  }

  // è·å–æ‰€æœ‰æ¨¡å‹
  public getAllModels(): AIModel[] {
    return Array.from(this.models.values())
  }

  // è®¾ç½®è¿æ¥çŠ¶æ€
  private setConnectionStatus(status: 'connected' | 'error' | 'unknown', errorMessage?: string): void {
    this._connectionStatus = status;
    if (status === 'error' && errorMessage) {
      this._errorMessage = errorMessage;
    } else if (status === 'connected') {
      this._errorMessage = null;
    }
  }
  
  // ä¸‹è½½æ¨¡å‹


  // è·å–ç‰¹å®šç±»å‹çš„æ¨¡å‹
  public getModelsByType(type: ModelType): AIModel[] {
    return Array.from(this.models.values()).filter((model) => model.type === type)
  }

  // è·å–å¯ç”¨çš„æ¨¡å‹
  public getAvailableModels(): AIModel[] {
    return Array.from(this.models.values()).filter((model) => model.status === "ready")
  }

  // è·å–æ¨¡å‹è¯¦æƒ…
  public getModel(modelId: string): AIModel | undefined {
    return this.models.get(modelId)
  }

  // ä¸‹è½½æ¨¡å‹
  public async downloadModel(modelId: string): Promise<ModelTask> {
    // æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ä¸‹è½½ä»»åŠ¡
    let task = this.modelTasks.get(modelId)
    if (task && ["pending", "downloading"].includes(task.status)) {
      return task
    }

    // åˆ›å»ºæ–°çš„ä¸‹è½½ä»»åŠ¡
    task = {
      id: `task_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
      modelId,
      type: "download",
      status: "pending",
      progress: 0,
      createdAt: new Date(),
      updatedAt: new Date(),
    }

    this.modelTasks.set(modelId, task)

    // æ›´æ–°æ¨¡å‹çŠ¶æ€
    const model = this.models.get(modelId)
    if (model) {
      this.models.set(modelId, { ...model, status: "downloading" })
    }

    // å¼€å§‹ä¸‹è½½
    this.startModelDownload(modelId, task)

    return task
  }

  // å¼€å§‹æ¨¡å‹ä¸‹è½½
  private async startModelDownload(modelId: string, task: ModelTask): Promise<void> {
    try {
      // æ›´æ–°ä»»åŠ¡çŠ¶æ€
      task.status = "downloading"
      task.startedAt = new Date()
      this.modelTasks.set(modelId, { ...task })

      console.log(`ğŸ”„ å¼€å§‹ä¸‹è½½æ¨¡å‹: ${modelId}`)

      // è°ƒç”¨Ollama APIä¸‹è½½æ¨¡å‹
      const response = await fetch(`${this.config.ollamaUrl}/api/pull`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({ name: modelId }),
      })

      if (!response.ok) {
        throw new Error(`ä¸‹è½½æ¨¡å‹å¤±è´¥: ${response.status}`)
      }

      // Ollama APIè¿”å›çš„æ˜¯æµå¼å“åº”ï¼Œéœ€è¦é€è¡Œè¯»å–
      const reader = response.body?.getReader()
      if (!reader) {
        throw new Error("æ— æ³•è¯»å–å“åº”æµ")
      }

      let receivedLength = 0
      let totalLength = 0

      while (true) {
        const { done, value } = await reader.read()

        if (done) {
          break
        }

        // è§£æè¿›åº¦ä¿¡æ¯
        const text = new TextDecoder().decode(value)
        const lines = text.split("\n").filter((line) => line.trim())

        for (const line of lines) {
          try {
            const data = JSON.parse(line)

            if (data.total && data.completed) {
              totalLength = data.total
              receivedLength = data.completed

              const progress = Math.round((receivedLength / totalLength) * 100)

              // æ›´æ–°ä»»åŠ¡è¿›åº¦
              task.progress = progress
              task.updatedAt = new Date()
              this.modelTasks.set(modelId, { ...task })

              // æ›´æ–°æ¨¡å‹çŠ¶æ€
              const model = this.models.get(modelId)
              if (model) {
                this.models.set(modelId, {
                  ...model,
                  status: "downloading",
                  downloadProgress: progress,
                })
              }
            }
          } catch (e) {
            // å¿½ç•¥è§£æé”™è¯¯
          }
        }
      }

      // ä¸‹è½½å®Œæˆï¼Œæ›´æ–°çŠ¶æ€
      task.status = "completed"
      task.progress = 100
      task.completedAt = new Date()
      this.modelTasks.set(modelId, { ...task })

      // æ›´æ–°æ¨¡å‹çŠ¶æ€
      await this.fetchOllamaModels() // é‡æ–°è·å–æ¨¡å‹åˆ—è¡¨ä»¥æ›´æ–°çŠ¶æ€

      console.log(`âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: ${modelId}`)
    } catch (error) {
      console.error(`âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: ${modelId}`, error)

      // æ›´æ–°ä»»åŠ¡çŠ¶æ€
      task.status = "failed"
      task.error = error instanceof Error ? error.message : "ä¸‹è½½å¤±è´¥"
      task.updatedAt = new Date()
      this.modelTasks.set(modelId, { ...task })

      // æ›´æ–°æ¨¡å‹çŠ¶æ€
      const model = this.models.get(modelId)
      if (model) {
        this.models.set(modelId, { ...model, status: "download_failed" })
      }
    }
  }

  // åˆ é™¤æ¨¡å‹
  public async deleteModel(modelId: string): Promise<boolean> {
    try {
      // æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
      const model = this.models.get(modelId)
      if (!model) {
        throw new Error(`æ¨¡å‹ä¸å­˜åœ¨: ${modelId}`)
      }

      // åªèƒ½åˆ é™¤Ollamaæ¨¡å‹
      if (model.provider !== "ollama") {
        throw new Error(`ä¸æ”¯æŒåˆ é™¤éOllamaæ¨¡å‹: ${modelId}`)
      }

      console.log(`ğŸ—‘ï¸ å¼€å§‹åˆ é™¤æ¨¡å‹: ${modelId}`)

      // è°ƒç”¨Ollama APIåˆ é™¤æ¨¡å‹
      const response = await fetch(`${this.config.ollamaUrl}/api/delete`, {
        method: "DELETE",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({ name: modelId }),
      })

      if (!response.ok) {
        throw new Error(`åˆ é™¤æ¨¡å‹å¤±è´¥: ${response.status}`)
      }

      // æ›´æ–°æ¨¡å‹çŠ¶æ€
      this.models.set(modelId, { ...model, status: "not_downloaded" })

      console.log(`âœ… æ¨¡å‹åˆ é™¤æˆåŠŸ: ${modelId}`)
      return true
    } catch (error) {
      console.error(`âŒ æ¨¡å‹åˆ é™¤å¤±è´¥: ${modelId}`, error)
      return false
    }
  }

  // è·å–æ¨¡å‹ä»»åŠ¡
  public getModelTasks(): ModelTask[] {
    return Array.from(this.modelTasks.values())
  }

  // è·å–æ¨¡å‹ä»»åŠ¡è¯¦æƒ…
  public getModelTask(taskId: string): ModelTask | undefined {
    // é€šè¿‡taskIdéå†æŸ¥æ‰¾ä»»åŠ¡
    for (const [id, task] of this.modelTasks.entries()) {
      if (task.id === taskId) {
        return task;
      }
    }
    return undefined;
  }

  // è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ›´æ–°ç‰ˆæœ¬ï¼ŒåŒ…å«providerç»Ÿè®¡ï¼‰
  public getModelStats(): ModelStats {
    const models = Array.from(this.models.values());
    return {
      total: models.length,
      ready: models.filter(m => m.status === 'ready').length,
      downloading: models.filter(m => m.status === 'downloading').length,
      notDownloaded: models.filter(m => m.status === 'not_downloaded').length,
      byType: {
        chat: models.filter(m => m.type === 'chat').length,
        code: models.filter(m => m.type === 'code').length,
        multimodal: models.filter(m => m.type === 'multimodal').length
      },
      byProvider: {
        ollama: models.filter(m => m.provider === 'ollama').length,
        openai: models.filter(m => m.provider === 'openai').length,
        anthropic: models.filter(m => m.provider === 'anthropic').length,
        google: models.filter(m => m.provider === 'google').length
      },
      totalSize: models.reduce((sum, model) => sum + model.size, 0)
    };
  }

  // ä½¿ç”¨æ¨¡å‹ï¼ˆè®°å½•ä½¿ç”¨æ—¶é—´ï¼‰
  public useModel(modelId: string): void {
    const model = this.models.get(modelId);
    if (model) {
      this.models.set(modelId, {
        ...model,
        lastUsed: new Date(),
        usageCount: (model.usageCount || 0) + 1
      });
    }
  }
}

// React Hook for Model Management
export const useModelManagement = (): {
  models: AIModel[];
  tasks: ModelTask[];
  stats: ReturnType<ModelManagementCenter['getModelStats']> | undefined;
  connectionStatus: 'connected' | 'error' | 'unknown';
  errorMessage: string | null;
  loading: boolean;
  refreshModels: () => Promise<void>;
  downloadModel: (modelId: string) => Promise<ModelTask>;
  deleteModel: (modelId: string) => Promise<void>;
  getRecommendedModels: (type: ModelType, limit?: number) => AIModel[];
} => {
  const [models, setModels] = useState<AIModel[]>([]);
  const [tasks, setTasks] = useState<ModelTask[]>([]);
  const [stats, setStats] = useState<ReturnType<ModelManagementCenter['getModelStats']> | undefined>();
  const [loading, setLoading] = useState<boolean>(true);
  const [updateInterval, setUpdateInterval] = useState<NodeJS.Timeout | null>(null);

  useEffect(() => {
    const modelManager = ModelManagementCenter.getInstance();
    
    // åˆå§‹åŠ è½½
    const loadModels = (): void => {
      const currentModels = modelManager.getAllModels();
      const currentTasks = modelManager.getAllTasks();
      const currentStats = modelManager.getModelStats();
      
      setModels(currentModels);
      setTasks(currentTasks);
      setStats(currentStats);
      setLoading(false);
    };

    loadModels();

    // è®¾ç½®å®šæœŸæ›´æ–°
    const interval = setInterval(loadModels, 1000);
    setUpdateInterval(interval);

    return () => {
      if (interval) {
        clearInterval(interval);
      }
    };
  }, []);

  const modelManager = ModelManagementCenter.getInstance();

  return {
    models,
    tasks,
    stats,
    connectionStatus: modelManager.connectionStatus,
    errorMessage: modelManager.errorMessage,
    loading,
    refreshModels: modelManager.refreshModels.bind(modelManager),
    downloadModel: modelManager.downloadModel.bind(modelManager),
    deleteModel: modelManager.deleteModel.bind(modelManager),
    getRecommendedModels: modelManager.getRecommendedModels.bind(modelManager)
  };
};

// æ·»åŠ ç¼ºå¤±çš„ç±»å‹å®šä¹‰
export type ModelStatus = "ready" | "downloading" | "not_downloaded" | "download_failed" | "unknown" | "unavailable"
export type TaskType = "download" | "update" | "delete"
export type TaskStatus = "pending" | "downloading" | "completed" | "failed"

export interface ModelStats {
  total: number
  ready: number
  downloading: number
  notDownloaded: number
  byType: Record<ModelType, number>
  byProvider: Record<ModelProvider, number>
  totalSize: number
}

// å¯¼å‡ºæ¨¡å‹ç®¡ç†ä¸­å¿ƒå®ä¾‹
export const modelManagementCenter = ModelManagementCenter.getInstance()

